#The following pipeline was produced by Natalie J. Forsdick, January 2020, based on work carried out in Chapter Three to conduct analysis of introgression using Genotyping By Sequencing (GBS) data as input. I gratefully acknowledge Denise Martini, Alana Alexander, and Hugh Cross for their advice and assistance with various steps presented herein. 

#This pipeline uses the command-line bash language, followed by analysis and visualisation in \textsf{R}. Notes and comments explaining steps within the pipeline are prefaced by the \# character. The various steps should be run independently of one another. Variables should be modified according to the project-specific directories and names of input and output files, and available software versions. 

########################################################################
1. DEMULTIPLEXING, FILTERING AND TRIMMING
########################################################################

# Firstly we must demultiplex the GBS data containing sequences from multiple samples.
# Requirements: Input FASTQ file of GBS data; a tab-delimited TXT file containing two columns, the first containing the barcode sequence and the second containing the respective sample file name to output.
# e.g., ACCATCG	SQ063_CCHB8ANXX_s_1_1fq.fastq

# Script name: GBS_preprocessing.sh
# To run: 
bash GBS_preprocessing.sh /path/to/datafile.fq /path/to/barcodes.txt /path/to/adapters.fa

# Script:

#!/bin/bash -e

# Check for arguments/help
if [ "$1" == "help" ]; then
	    printf "\nThis is a script to demultiplex, filter, and trim raw GBS data using sabre and cutadapt\n"
	    printf "Three arguments are needed: the path to the FASTQ file containing raw GBS data #,\na the path to the tab-delimited text file containing sample barcodes and output file names,\nand the path to a FASTA file containing adapter sequences for removal."
	    printf "For example:\n\t./GBS_preprocessing.sh /path/to/datafile.fq /path/to/barcodes.txt /path/to/adapters.fa\n\n"
	    exit 1

elif [ "$1" != "" ]; then
	    printf "\tThe raw data file is:  ${1}\n"
else
	printf "\tYou have not specified a file with raw GBS data for processing\n\n"
			    exit 1
fi

if [ "$2" != "" ]; then
	    printf "\tThe barcodes file is:  ${2}\n"
else
	printf "\tYou have not specified a barcodes file\n\n"
			    exit 1
fi

if [ "$3" != "" ]; then
	    printf "\tThe adapter file is:  ${3}\n"
else
	printf "\tYou have not specified aa adapter file\n\n"
			    exit 1
fi

# Setting up a logfile to collect output metrics
start=`date`
echo "Logfile for GBS pipeline run on $start" > logfile1.txt
logfile=./logfile1.txt

echo "Demultiplexing with sabre"
echo "Demultiplexing with sabre" >> $logfile

# Make output directory
mkdir ./demultiplexed
cd ./demultiplexed/

# Command to run sabre v1.0 for single end data, uncomment the option needed: -m 1 allows for 1 mismatch in barcode.
# -b = input file of sample barcodes, -u = output file of sequences with unknown barcodes.
sabre se -f $1 -b $2 -u unknown_barcode.fq > sabre_summary.txt
#sabre se -f ${datadir}${datafile} -m 1 -b ${datadir}SQ0683_barcodes.txt -u unknown_barcode.fq > ${datadir}/sabre_summary.txt

echo "Demultiplexing complete"
echo "Demultiplexing complete" >> $logfile
echo "Cleaning and defining sample list"

# Calling a sample list for the next step from the demultiplexed files here
samplist=`ls -1 *.fastq | sed 's/.fastq//'`

cd ..

##########

# Next we filter out data that does not contain the correct enzyme restriction site.

echo "Filtering and trimming with cutadapt"
echo "Filtering and trimming with cutadapt" >> $logfile

mkdir ./filtered
echo "Filtering summary" > ./filtering_summary.txt
mkdir ./trimmed50
echo "Trimming summary" > ./trimming_summary.txt

for samp in $samplist
do

now=`date`
echo "Processing $samp $now" >> $logfile
echo "Filtering $samp"

cd ./filtered/

echo "$samp" >> ../filtering_summary.txt
# First we count the number of reads in the sequence file.
grep '^@' ../demultiplexed/${samp}.fastq | wc -l >> ../filtering_summary.txt
## Next we select only the reads that begin with the proper enzyme restriction site. This should be adapted to your sequencing outputs.
grep -B1 -A2 '^TGCAG' ../demultiplexed/${samp}.fastq | sed '/^--$/d' > ${samp}.fastq
# We then count the number of reads with the correct enzyme restriction site.  
grep '^@' ${samp}.fastq | wc -l >> ../filtering_summary.txt

cd ..

##########

# Next we trim the filtered sequence reads to remove adapters using CUTADAPT v1.17.

echo "Trimming $samp"
cd ./trimmed50/

echo "$samp" >> ../trimming_summary.txt
grep '^@' ../filtered/${samp}.fastq | wc -l >> ../trimming_summary.txt

# Run cutadapt - this requires a FASTA file containing the appropriate adapter sequence(s).
# Reads shorter than (-m) 50 following adapter removal are removed.
cutadapt -a file:$3 -m 50 -o ${samp}.fastq ../filtered/${samp}.fastq >> ../cutadapt_summary1.txt

# Output the number of reads remaining.
grep '^@' ${samp}.fastq | wc -l >> ../trimming_summary.txt

cd ..

echo "$samp processed"
echo "$samp processed" >> $logfile
done

end=`date`
echo "Filtering and trimming done"
echo "Filtering and trimming done $end" >> $logfile

# This ends Step 1. All logfiles and TXT files of metrics should be inspected for errors or anomalies before continuing.

########################################################################
2. SEQUENCE MAPPING
########################################################################

# Here we map the processed sequence reads to the kakī reference genome.

# Script name: GBS_mapping.sh
# This requires two arguments to run: first, a text file with a list of FASTQ files to map, and second, the path to the selected reference genome in FASTA format. You must cd into the directory containing the FASTQ files before running the script: 
bash GBS_mapping.sh /path/to/fqlist.txt /path/to/reference_genome.fasta

# Script:

#!/bin/bash -e

# Set up environment and variables
module load samtools/1.9 bwa/0.7.17

# Check for arguments/help
if [ "$1" == "help" ]; then
	    printf "\nThis is a script to map multiple sequence files to a single reference using BWA\n"
	    printf "Two arguments are needed: a text file with a list of fq files to map,\nand the reference sequence to map against\n"
	    printf "For example:\n\t./simpler_mapping_loop.sh SEQUENCE_LIST /path/to/ref/REFERENCE.fasta\n\n"
	    exit 1

elif [ "$1" != "" ]; then
	    printf "\tThe list file is:  ${1}\n"
else
	printf "\tYou have not specified a file with list of sequences to map\n\n"
			    exit 1
fi

if [ "$2" != "" ]; then
	    printf "\tThe reference file is:  ${2}\n"
else
	printf "\tYou have not specified a reference file\n\n"
			    exit 1
fi

# Ensure the reference genome is indexed.
if [ ! -f $2\.amb ]
	then
	printf "\n\tThe reference has not been indexed. Indexing now\n"
	bwa index -a bwtsw $2
else
	echo "BWA index file found" 
fi

start=`date`
echo "Beginning sequence mapping at $start"

# Then we run the mapping loop for all samples
cat $1 | while read fq
do

	echo $fq
	base=$(echo $fq | cut -f 1 -d '.')
	# Map reads, convert to BAM format, sort by genome location, index outputs for downstream processes.
	# -t = number of threads, -b = output in BAM format, -h = output header, -o = output filename
	bwa mem -t 4 $2 $fq | samtools view -b -h - | samtools sort - -o sorted_$base\.bam
	samtools index sorted_$base\.bam
done

end=`date`
echo "Variant discovery completed at $now"

##########

# Following mapping, we collect the percentage of mapped/unmapped reads from the output BAM files.
for bam in $(ls *.bam)
do

	echo $bam

    map=$(samtools view -F4 -c $bam)
    unmap=$(samtools view -f4 -c $bam)
    total=$(($map + $unmap))
    perc_mapped=`echo "scale=4;($map/$total)*100" | bc`

    echo $bam >> mappingv2.3_logfile.txt
    echo "mapped $map" >> mappingv2.3_logfile.txt
    echo "% mapped $perc_mapped" >> mappingv2.3_logfile.txt
    echo "unmapped $unmap" >> mappingv2.3_logfile.txt
done

# Step 2 is complete. Logfiles containing mapping reports should be inspected before proceeding to Step 3. 

########################################################################
3. VARIANT DISCOVERY
########################################################################

# The next phase is variant discovery. Here we use the STACKS reference-guided pipeline for variant discovery. 
# Requirements: A tab-delimited text file containing a list of sample files and their associated population of origin.
# Script name: STACKS_varcalling.sh
# To run:
bash STACKS_varcalling.sh /path/to/processed/samples/ /path/to/reference/ /path/to/ref_population.txt

# Script:

#!/bin/bash -e

if [ "$1" == "help" ]; then
	    printf "\nThis is a script to call variants for a set of processed sample files using STACKS\n"
	    printf "Three arguments are needed: the path to the directory containing the processed sample files,\nthe path to the reference genome,\nand the populations.txt file\n"
	    printf "For example:\n\t./STACKS_varcalling.sh /path/to/processed/samples/ /path/to/output/directory/ /path/to/ref_population.txt\n\n"
	    exit 1

elif [ "$1" != "" ]; then
	    printf "\tThe directory containing processed sample files is:  ${1}\n"
else
	printf "\tYou have not specified a directory containing processed sample files\n\n"
			    exit 1
fi

if [ "$2" != "" ]; then
	    printf "\tThe reference file is:  ${2}\n"
else
	printf "\tYou have not specified a reference file\n\n"
			    exit 1
fi

if [ "$3" != "" ]; then
	    printf "\tThe populations file is:  ${3}\n"
else
	printf "\tYou have not specified a populations file\n\n"
			    exit 1
fi

# Set up environment variables
module load stacks/2.2

start=`date`
echo "Beginning variant discovery at $start"
# -d can be used to perform a 'dry-run' to test the pipeline without performing variant calling.
ref_map.pl -T 10 —-samples $1 -o $2 —-popmap $3 #-d
end=`date`
echo "Variant discovery completed at $now"

# Step 3 completed. This should produce a VCF containing variant call data for all individuals specified in the populations file. The VCF should be checked to ensure the correct number of individuals is included, and that general formatting appears correct before proceeding to Step 4.

########################################################################
4. VARIANT FILTERING
########################################################################

# Next we want to filter variants to produce a robust set of biallelic SNPs for downstream analyses. 
# This requires output results to be considered step-by-step so filtering can be appropriately tailored to the data. Thus, this is not set up as a script, but as a series of line-by-line commands. 
# This step draws on the filtering pipeline described in ddocent.com/filtering, and the options available with VCFTOOLS.

# Setting up environment variables
module load vcftools/0.1.15
# export PERL5LIB=/usr/local/vcftools_0.1.15/share/perl # May need to uncomment this if any problems with VCFTOOLS.

# First let's find out how many variants were produced from the STACKS pipeline.
grep "^[^#;]" /path/to/variants.vcf | wc -l

# Now let's do some filtering. This could be run with each filter separately, where the number of variants remaining is checked at each stage, but it is more efficient to run it all at once. If very low (<10K) variants are produced at the end, it may be better to go back and investigate what is happening at each step. 
# Here the variable to alter each time is the names of the input and output VCFs.
# First let's remove indels, and make sure we only have biallelic SNPs.
vcftools --vcf /path/to/pop.vcf --remove-indels --min-alleles 2 --max-alleles 2 --remove-filtered-all --recode --out /path/to/filt_variants1
grep "^[^#;]" /path/to/filt_variants1.recode.vcf | wc -l

# Next we can remove sites with more than 10% missing data.
vcftools --vcf /path/to/filt_variants1.recode.vcf --max-missing 0.1 --remove-filtered-all --recode --out /path/to/filt_variants2
grep "^[^#;]" /path/to/filt_variants2.recode.vcf | wc -l

# Next we want to check whether any samples have high levels of missing data - likely any individuals with low number of raw sequences or low number of mapped sequences will be an issue. 
# Let's check the numbers and determine an appropriate cut-off.
vcftools filt_variants2.recode.vcf --missing-indv
# This makes a file called out.imiss, where individual missingness is listed.
# Examine the file
less out.imiss

# We can visualise the distribution of missing data per individual on the command line.
mawk '!/IN/' out.imiss | cut -f5 > totalmissing
gnuplot << \EOF 
set terminal dumb size 120, 30
set autoscale 
unset label
set title "Histogram of % missing data per individual"
set ylabel "Number of Occurrences"
set xlabel "% of missing data"
#set yr [0:100000]
binwidth=0.01
bin(x,width)=width*floor(x/width) + binwidth/2.0
plot 'totalmissing' using (bin($1,binwidth)):(1.0) smooth freq with boxes
pause -1
EOF

# We want to tailor individual filtering to suit the distribution of the data, but removing individuals with > 50% missing data is probably a good way to start. Let's make a list of those individuals wwith >50% missing data and remove them from the set.
mawk '$5 > 0.5' out.imiss | cut -f1 > lowDP.indv
vcftools --vcf filt_variants2.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out filt_variants2_lowDPrem.recode.vcf

# At this point further exploratory analysis of the SNP set should be conducted - individual missingness, SNP density, depth, heterozygosity, HWE, population differentiation can all be assessed. Do the patterns make sense given the biological information? Are there any anomalies? What filtering is appropriate for the questions you aim to answer by downstream analyses, and what number of SNPs is required to answer these questions? See VCFTOOLS manual for detailed instructions, and ddocent.com/filtering for suggestions. 

# The following is just an example, filtering on a minimum minor allele frequency of 0.01, retaining SNPs with depth between 5-200, and a minimum site quality of 20.
vcftools --vcf /path/to/filt_variants2_lowDPrem.recode.vcf --maf 0.01 --minDP 5 --maxDP 200 --minQ 20 --remove-filtered-all --recode --out /path/to/filt_variants3_lowDPrem 
grep "^[^#;]" /path/to/filt_variants3_lowDPrem.recode.vcf | wc -l

# We also want to remove loci on the sex chromosomes, to avoid problems associated with sex-linked or haploid loci. Future work can incorporate further analyses of these loci -> perhaps independent analysis of population clustering, or appropriate use of tools that can incorporate haploid sites.
vcftools --vcf /path/to/filt_variants3_lowDPrem.recode.vcf --not-chr CHROM_Z --not-chr CHROM_W --remove-filtered-all --recode --out /path/to/filt_variants3_lowDPrem_SexChromrem.recode.vcf
grep "^[^#;]" /path/to/filt_variants3_lowDPrem_SexChromrem.recode.vcf | wc -l

# This completes Step 4. We should now have a good understanding of our SNP set, and be confident that it can produce robust results in downstream analyses.

########################################################################
5. PRE-PROCESSING FOR DOWNSTREAM ANALYSIS OF INTROGRESSION
########################################################################

# Here we need to convert the VCF file to be passed as input to ADMIXTURE (or other analyses pipelines as required).

# Here we also run these as simple commands in the terminal
# Requirements: We need to create a new populations text file ensuring that filtered individuals are removed. 

# Set up the environmental variables
module load stacks/2.2
module load plink/1.9

# -V = VCF input, -O = output directory, -M = populations text file, -t = threads. Here we output to PLINK format - output files will include PLINK format MAP and PED files
populations -V filt_variants3_lowDPrem_SexChromrem.recode.vcf -O ./ -M negfree_ref_population.txt -t 8 --plink
# This process using STACKS produces useful estimates for population level metrics including average individuals per site, heterozygosity, population-specific variants/polymorphic sites, and private alleles.

# The output PLINK format files are then passed to PLINK for conversion to BED, BIM, and FAM files that will be passed as input to ADMIXTURE.
plink --file filt_variants3_lowDPrem_SexChromrem.recode.plink --make-bed --aec --out filt_variants3_lowDPrem_SexChromrem
# Following conversion, the BIM file requires modification before passing to ADMIXTURE. Chromosome/scaffold names cannot begin with a numeric, so add 'C' to the start; also the names cannot include '_', so these need removing from column one.

# If we want to investigate population clustering with ADEGENET in R, we need to convert those same PLINK files to produce a RAW format file. 
plink --file filt_variants3_lowDPrem_SexChromrem.recode.plink --aec --recode A 
# Pull down RAW and MAP files for input to ADEGENET. Remove header of MAP files, and sort RAW file by population before passing to ADEGENET.

# This completes Step 5, our files are now ready to be analysed for evidence of introgression.

########################################################################
6. ANALYSIS OF INTROGRESSION
########################################################################

# While all previous steps can be run simply on a standard computing cluster or high-performance computer, ADMIXTURE analysis is conducted on a high-capacity computing cluster. Here we used NeSI's (New Zealand eScience Infrastructure) mahuika, which uses a SLURM system for queuing jobs.
# To take full advantage of the high-capacity system and maximise efficiency, and perform multiple ADMIXTURE runs simultaneously which can then be combined for robust results, we use an array to run this. Completing a full run comprising ~15K - ~140K SNPs took <2 hrs real-time. 

# Script name: admixture.sl
# This requires the output files from conversion to be present (BED, BIM, and FAM) in the directory. 

# To run:
ssh mahuika
# Copy script to the directory, then run:
sbatch admixture.sl  

# Script:

#!/bin/bash -e
#SBATCH -A [NeSI_account_code] 
#SBATCH -J admix1 # Job name
#SBATCH --time 02:00:00 # real time requested
#SBATCH -N 1 # number of nodes
#SBATCH -c 36 # number of cpus per task
#SBATCH -n 1 # number of tasks
#SBATCH --mem=100G # maximum memory required
#SBATCH --array=1-100 # setting up as an array - all 100 ADMIXTURE runs will be performed simultaneously
#SBATCH --partition=large # partition type
#SBATCH --mail-user=[user_email] # to receive notifications to
#SBATCH --mail-type=ALL
#SBATCH --output admix1.%j.out
#SBATCH --error admix1.%j.err

module purge # Clear existing environmental variables
mkdir /path/to/output/directory/admixture_${SLURM_ARRAY_TASK_ID}
cd /path/to/output/directory/admixture_${SLURM_ARRAY_TASK_ID}

admixture=/path/to/admixture_v1.3/
start=`date`
echo "Beginning ADMIXTURE analysis at $start"

# We want to test up to three clusters in the streamlined pipeline, to be sure no additional population structuring is detected among kakī, and that k=2 remains the most appropriate number of clusters to use. 
# -C = termination criterion - set to terminate when the log-likelihood increases by less than default of 0.0001 between iterations, --cv = perform (default) 5-fold cross-validation to assess appropriate number of clusters, -s = seed (here we base this on the current time), -j = number of threads.
for k in {1..3};
do
	$admixture -C 0.0001 --cv /path/to/filt_variants3_lowDPrem_SexChromrem.bed $k -s time -j36 | tee log_filt_variants3_lowDPrem_SexChromrem_${k}.out
done

end=`date`
echo "ADMIXTURE analysis completed at $now"

# Step 6 is completed. Barring errors or further adjustments, we can now proceed to Step 7.

########################################################################
7. PROCESSING ADMIXTURE OUTPUTS
########################################################################

# In the first step, we want to ensure that all the outputs are individually named, so that when we pull these down, they don't get overwritten.
# We rename Q files based on the run number.
# We use 'ls' to check that the renaming is correctly before replacing this with 'mv' to rename the files.
for k in {1..3}
do
	cd admixture_$k
	for i in *.Q
	do
		ls $i ${k}_${i} # once correct naming is confirmed, replace 'ls' with 'mv'
	done
	cd ../
done

# Then we want to move these all into one folder
for k in {1..3}
do
	for i in admixture_${k}/*.Q
	do
		ls $i admixture_Q/ # again, once correct movement is confirmed, replace 'ls' with 'mv'
	done
done

# We then want to extract the CV values to confirm appropriate number of clusters. We pull out CV values sorted by k.
grep "^CV" admixture_*.out | sort -k 3 > admixture_CV_sorted.txt

# This is the end of Step 7. These files can then be pulled down to the local system ready for processing and visualisation with the R package 'Pophelper'. 

########################################################################


